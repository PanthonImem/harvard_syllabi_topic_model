{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c98f5d-f284-4586-95b9-8cbc125782b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import plotly.graph_objects as go\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b58dbd-6fc6-406b-bdd0-0788d9089afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf = pd.read_csv('processed_data/computation_courses.csv').drop_duplicates('course name').sort_values('course name')\n",
    "documents = alldf['cleaned_text'].values\n",
    "alldf['course department2'] = alldf['course department'] \n",
    "alldf.loc[~alldf['course department'].isin(['MATH','STAT','APMTH','BIOSTAT','APCOMP']),'course department2'] = 'OTHERS'\n",
    "departments = alldf['course department2'].values\n",
    "alldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dd4af-07b7-4e82-bcf5-a86509ab08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "def get_top_words_with_department_dataframe(documents, departments, stop_words, top_n=10):\n",
    "    if len(documents) != len(departments):\n",
    "        raise ValueError(\"Documents and departments must have the same length.\")\n",
    "    \n",
    "    # Initialize CountVectorizer with custom preprocessing to remove stopwords\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "    \n",
    "    # Fit and transform the documents to term-document matrix\n",
    "    term_matrix = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Sum term counts across all documents\n",
    "    term_totals = term_matrix.sum(axis=0)\n",
    "    term_totals = {word: term_totals[0, idx] for word, idx in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # Calculate document frequency (number of documents containing each term)\n",
    "    doc_freqs = (term_matrix > 0).sum(axis=0)\n",
    "    doc_freqs = {word: doc_freqs[0, idx] for word, idx in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # Calculate department-wise breakdown\n",
    "    department_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for idx, doc in enumerate(documents):\n",
    "        department = departments[idx]\n",
    "        for word in set(doc.lower().split()):  # Use set to avoid duplicate word counts in a document\n",
    "            if word in vectorizer.vocabulary_:\n",
    "                department_counts[word][department] += 1\n",
    "    \n",
    "    # Combine term totals, document frequencies, and department breakdowns into DataFrame format\n",
    "    rows = []\n",
    "    all_departments = sorted(set(departments))  # Get unique department names\n",
    "    for word in vectorizer.vocabulary_:\n",
    "        row = {\n",
    "            'Word': word,\n",
    "            'Total Count': term_totals[word],\n",
    "            'Document Frequency': doc_freqs[word]\n",
    "        }\n",
    "        for dept in all_departments:\n",
    "            row[dept] = department_counts[word].get(dept, 0)\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame and sort\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values(by=['Total Count', 'Document Frequency'], ascending=False).head(top_n)\n",
    "    return df\n",
    "\n",
    "# Get top words with department breakdown as DataFrame\n",
    "top_words_df = get_top_words_with_department_dataframe(documents, departments, stop_words, top_n=100).reset_index(drop = True)\n",
    "\n",
    "top_words_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff3212-7071-4983-a1e8-f13bcc5373c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 #number of words for coherence score\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "coherence_scores = []\n",
    "perplexities = []\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=10,\n",
    "    stop_words='english',\n",
    "    token_pattern=r'(?u)\\b[A-Za-z]+\\b'  # Only keep words with alphabetic characters\n",
    ")\n",
    "doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Define function to calculate coherence score\n",
    "def coherence_score(topic_words, doc_term_matrix):\n",
    "    scores = []\n",
    "    for words in topic_words:\n",
    "        word_indices = [vectorizer.vocabulary_[w] for w in words if w in vectorizer.vocabulary_]\n",
    "        sub_matrix = doc_term_matrix[:, word_indices].toarray()\n",
    "        similarities = cosine_similarity(sub_matrix.T)\n",
    "        upper_tri_indices = np.triu_indices_from(similarities, k=1)\n",
    "        scores.append(similarities[upper_tri_indices].mean())\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Loop through different numbers of topics\n",
    "num_topics_range = np.arange(3, 10)\n",
    "for num_topics in num_topics_range:\n",
    "    \n",
    "    \n",
    "    # Fit LDA\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    \n",
    "    # Extract topic-word distributions\n",
    "    topic_word_distributions = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "    top_words = np.argsort(topic_word_distributions, axis=1)[:, -N:]\n",
    "    vocabulary = np.array(vectorizer.get_feature_names_out())\n",
    "    topic_top_words = vocabulary[top_words]\n",
    "    \n",
    "    # Calculate coherence score\n",
    "    coherence = coherence_score(topic_top_words, doc_term_matrix)\n",
    "    coherence_scores.append(coherence)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = lda.perplexity(doc_term_matrix)\n",
    "    perplexities.append(perplexity)\n",
    "    \n",
    "    print(f\"Topics: {num_topics}, Coherence: {np.round(coherence, 3)}, Perplexity: {np.round(perplexity, 1)}\")\n",
    "\n",
    "# Find indices of minimum/maximum values\n",
    "min_perplexity_idx = np.argmin(perplexities)\n",
    "max_coherence_idx = np.argmax(coherence_scores)\n",
    "\n",
    "# Plot metrics in subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot Coherence Score\n",
    "axes[0].plot(num_topics_range, coherence_scores, label='Coherence Score', marker='o', color='b')\n",
    "axes[0].scatter(num_topics_range[max_coherence_idx], coherence_scores[max_coherence_idx], color='green', s=100, label='Max Coherence', zorder=5)\n",
    "axes[0].set_title(\"Coherence Score vs Number of Topics\")\n",
    "axes[0].set_xlabel(\"Number of Topics\")\n",
    "axes[0].set_ylabel(\"Coherence Score\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "\n",
    "# Plot Perplexity\n",
    "axes[1].plot(num_topics_range, perplexities, label='Perplexity', marker='o', color='r')\n",
    "axes[1].scatter(num_topics_range[min_perplexity_idx], perplexities[min_perplexity_idx], color='green', s=100, label='Min Perplexity', zorder=5)\n",
    "axes[1].set_title(\"Perplexity vs Number of Topics\")\n",
    "axes[1].set_xlabel(\"Number of Topics\")\n",
    "axes[1].set_ylabel(\"Perplexity\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38845b13-4d36-4eab-b481-d859bee7365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the topics\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {idx + 1}: \", [feature_names[i] for i in topic.argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6da65-3952-4203-8b43-1e2c1df37bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic distribution for each document\n",
    "doc_topic_distribution = lda.transform(doc_term_matrix)  # Shape: [n_docs, n_topics]\n",
    "\n",
    "# Find the most representative document for each topic\n",
    "def find_representative_documents(doc_topic_distribution, top_n=1):\n",
    "    \"\"\"\n",
    "    Find the most representative documents for each topic.\n",
    "\n",
    "    Parameters:\n",
    "        doc_topic_distribution: ndarray\n",
    "            The topic distribution for each document (output of LDA's `transform` method).\n",
    "        top_n: int\n",
    "            Number of top documents to find for each topic.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are topic IDs and values are lists of document indices.\n",
    "    \"\"\"\n",
    "    n_topics = doc_topic_distribution.shape[1]\n",
    "    representative_docs = {}\n",
    "\n",
    "    for topic_id in range(n_topics):\n",
    "        # Get the indices of the top-n documents for this topic\n",
    "        top_docs = np.argsort(doc_topic_distribution[:, topic_id])[-top_n:][::-1]\n",
    "        representative_docs[topic_id] = top_docs.tolist()\n",
    "\n",
    "    return representative_docs\n",
    "\n",
    "# Find the most representative documents for each topic\n",
    "top_n = 3  # Change to 2, 3, etc., for more representative documents\n",
    "representative_docs = find_representative_documents(doc_topic_distribution, top_n)\n",
    "\n",
    "# Display results\n",
    "for topic, docs in representative_docs.items():\n",
    "    print(f\"Topic {topic+1}: Document indices {docs}\")\n",
    "    for doc in docs:\n",
    "        print(documents[doc][:100])\n",
    "        print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecaa85-1b2d-41ed-b5ba-c94aa7dd220d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289975af-723e-44e9-ad51-40137081d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_doc_topic_distribution = pd.DataFrame(doc_topic_distribution).round(2)\n",
    "ldf2 = pd.concat([alldf.reset_index(drop = True), rounded_doc_topic_distribution], axis=1, ignore_index=True)\n",
    "ldf2.columns = list(alldf.columns) + [f\"topic {i}\" for i in range(1, 6)]\n",
    "df = ldf2[ldf2['course name'].str.contains('')][['course name','Name']+[f\"topic {i}\" for i in range(1, 6)]]\\\n",
    ".sort_values('topic 5', ascending = False)\n",
    "df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b04cb-f3d2-4b58-a4c2-4fdfd3758751",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf2[ldf2['course department'] == 'STAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154909d-0fef-488e-9252-1880a7fbfd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate exclusivity of words across topics\n",
    "def get_anchor_words_exclusive(topic_word_matrix, feature_names, top_n=10, exclusivity_threshold=0.8):\n",
    "    n_topics, n_words = topic_word_matrix.shape\n",
    "    anchor_words = {}\n",
    "\n",
    "    # Normalize topic-word probabilities to convert them into probabilities\n",
    "    topic_word_probs = topic_word_matrix / topic_word_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # For each topic, find anchor words\n",
    "    for topic_idx in range(n_topics):\n",
    "        topic_probs = topic_word_probs[topic_idx]\n",
    "        exclusivity_scores = topic_probs / topic_word_probs.sum(axis=0)  # Score for each word\n",
    "        \n",
    "        # Select words based on exclusivity scores and top probabilities\n",
    "        candidate_indices = topic_probs.argsort()[::-1]  # Sort words by topic probability\n",
    "        exclusive_words = [\n",
    "            feature_names[i]\n",
    "            for i in candidate_indices\n",
    "            if exclusivity_scores[i] >= exclusivity_threshold\n",
    "        ][:top_n]\n",
    "        \n",
    "        anchor_words[f\"Topic {topic_idx+1}\"] = exclusive_words\n",
    "\n",
    "    return anchor_words\n",
    "\n",
    "# Example usage: Assuming 'lda' is the trained LDA model and 'vectorizer' is the CountVectorizer\n",
    "topic_word_matrix = lda.components_  # Shape: [n_topics, n_words]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Parameters\n",
    "top_n = 50\n",
    "exclusivity_threshold = 0.9  # A word must have at least 80% exclusivity for one topic to qualify\n",
    "\n",
    "anchor_words = get_anchor_words_exclusive(topic_word_matrix, feature_names, top_n, exclusivity_threshold)\n",
    "\n",
    "# Display anchor words\n",
    "for topic, words in anchor_words.items():\n",
    "    print(f\"{topic}: {', '.join(words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a290264-e58b-4d7e-8b38-1d3ad7829753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Determine the dominant topic for each document\n",
    "dominant_topics = np.argmax(doc_topic_distribution, axis=1)  # Majority topic for each document\n",
    "\n",
    "# Step 3: Add dominant topic to DataFrame for reference\n",
    "df['Dominant Topic'] = dominant_topics\n",
    "\n",
    "# Step 4: Reduce topic distribution to 2D using PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "topic_distribution_2d = pca.fit_transform(doc_topic_distribution)  # Shape: [n_docs, 2]\n",
    "\n",
    "# Step 5: Define a color palette for topics\n",
    "num_topics = lda.n_components\n",
    "colors = [f\"hsl({i * 360 / num_topics}, 70%, 50%)\" for i in range(num_topics)]  # HSL for distinct colors\n",
    "\n",
    "# Step 6: Create an interactive scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add scatter plot, grouping by dominant topic for coloring\n",
    "for topic_id in range(num_topics):\n",
    "    indices = dominant_topics == topic_id\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=topic_distribution_2d[indices, 0],\n",
    "        y=topic_distribution_2d[indices, 1],\n",
    "        mode='markers',\n",
    "        name=f\"Topic {topic_id}\",\n",
    "        text=df.loc[indices, 'Name'],  # Use course names for hover text\n",
    "        hovertemplate=(\n",
    "            \"<b>Course Name:</b> %{text}<br>\" +\n",
    "            \"<b>Topic:</b> %{meta}\"  # Include topic number in hover\n",
    "        ),\n",
    "        meta=[f\"Topic {topic_id}\" for _ in range(sum(indices))],\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=colors[topic_id],\n",
    "            line=dict(width=1, color='black')\n",
    "        )\n",
    "    ))\n",
    "\n",
    "# Step 7: Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Topic Distribution of Documents (PCA-reduced)\",\n",
    "    xaxis_title=\"Principal Component 1\",\n",
    "    yaxis_title=\"Principal Component 2\",\n",
    "    template=\"plotly\",\n",
    "    showlegend=True,\n",
    "    width=1000,  # Width of the plot (in pixels)\n",
    "    height=600   # Height of the plot (in pixels)\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aecbe8-f591-4da5-ada4-75e2ba51626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Determine the dominant topic for each document\n",
    "dominant_topics = np.argmax(doc_topic_distribution, axis=1)  # Majority topic for each document\n",
    "\n",
    "# Step 3: Add dominant topic to DataFrame for reference\n",
    "df['Dominant Topic'] = dominant_topics\n",
    "\n",
    "# Step 4: Reduce topic distribution to 3D using PCA for visualization\n",
    "pca = PCA(n_components=3)\n",
    "topic_distribution_3d = pca.fit_transform(doc_topic_distribution)  # Shape: [n_docs, 3]\n",
    "\n",
    "# Step 5: Define a color palette for topics\n",
    "num_topics = lda.n_components\n",
    "colors = [f\"hsl({i * 360 / num_topics}, 70%, 50%)\" for i in range(num_topics)]  # HSL for distinct colors\n",
    "\n",
    "# Step 6: Create an interactive 3D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add scatter plot, grouping by dominant topic for coloring\n",
    "for topic_id in range(num_topics):\n",
    "    indices = dominant_topics == topic_id\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=topic_distribution_3d[indices, 0],\n",
    "        y=topic_distribution_3d[indices, 1],\n",
    "        z=topic_distribution_3d[indices, 2],\n",
    "        mode='markers',\n",
    "        name=f\"Topic {topic_id+1}\",\n",
    "        text=df.loc[indices, 'Name'],  # Use document names for hover text\n",
    "        hovertemplate=(\n",
    "            \"<b>Document Name:</b> %{text}<br>\" +\n",
    "            \"<b>Topic:</b> %{meta}\"  # Include topic number in hover\n",
    "        ),\n",
    "        meta=[f\"Topic {topic_id+1}\" for _ in range(sum(indices))],\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=colors[topic_id],\n",
    "            line=dict(width=1, color='black')\n",
    "        )\n",
    "    ))\n",
    "\n",
    "# Step 7: Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Topic Distribution of Documents by Department (PCA-reduced to 3D)\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Principal Component 1\",\n",
    "        yaxis_title=\"Principal Component 2\",\n",
    "        zaxis_title=\"Principal Component 3\"\n",
    "    ),\n",
    "    template=\"plotly\",\n",
    "    showlegend=True,\n",
    "    width=800,  # Width of the plot (in pixels)\n",
    "    height=600,  # Height of the plot (in pixels)\n",
    "    font=dict(\n",
    "        size=9  # Corrected key for setting font size\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7b641-81ec-42d0-9124-21e5392ef8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e44a3-96ff-4b26-80ad-01821c8f9205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Step 2: Reduce topic distribution to 3D using PCA for visualization\n",
    "pca = PCA(n_components=3)\n",
    "topic_distribution_3d = pca.fit_transform(doc_topic_distribution)  # Shape: [n_docs, 3]\n",
    "\n",
    "# Step 3: Define a color palette for unique course departments\n",
    "unique_departments = alldf['course department2'].unique()\n",
    "num_departments = len(unique_departments)\n",
    "colors = [\n",
    "    \"#1f77b4\",  # Blue\n",
    "    \"#ff7f0e\",  # Orange\n",
    "    \"#2ca02c\",  # Green\n",
    "    \"#d62728\",  # Red\n",
    "    \"#bcbd22\",  # Gold\n",
    "    \"#8c564b\"   # Brown\n",
    "]\n",
    "# Step 4: Create an interactive 3D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add scatter plot, grouping by course department\n",
    "for i, department in enumerate(unique_departments):\n",
    "    indices = alldf['course department2'] == department\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=topic_distribution_3d[indices, 0],\n",
    "        y=topic_distribution_3d[indices, 1],\n",
    "        z=topic_distribution_3d[indices, 2],\n",
    "        mode='markers',\n",
    "        name=department,  # Use department name as legend\n",
    "        text=alldf.loc[indices, 'Name'],  # Use document names for hover text\n",
    "        hovertemplate=(\n",
    "            \"<b>Document Name:</b> %{text}<br>\" +\n",
    "            \"<b>Department:</b> %{meta}\"  # Include department name in hover\n",
    "        ),\n",
    "        meta=[department for _ in range(sum(indices))],\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=colors[i],\n",
    "            line=dict(width=1, color='black')\n",
    "        )\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Topic Distribution of Documents by Department (PCA-reduced to 3D)\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Principal Component 1\",\n",
    "        yaxis_title=\"Principal Component 2\",\n",
    "        zaxis_title=\"Principal Component 3\"\n",
    "    ),\n",
    "    template=\"plotly\",\n",
    "    showlegend=True,\n",
    "    width=800,  # Width of the plot (in pixels)\n",
    "    height=600,  # Height of the plot (in pixels)\n",
    "    font=dict(\n",
    "        size=9  # Corrected key for setting font size\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0820c2-9d96-41e6-bf30-e58231284ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Step 1: Load a pre-trained model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # A lightweight, efficient sentence embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: Define a function for sentence embeddings\n",
    "def get_sentence_embedding(sentences):\n",
    "    # Tokenize input sentences\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the embeddings\n",
    "    # Use the [CLS] token or mean pooling\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    sentence_embeddings = mean_pooling(embeddings, attention_mask)\n",
    "    return sentence_embeddings\n",
    "\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    # Perform mean pooling on token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "#embeddings = get_sentence_embedding(list(documents))\n",
    "\n",
    "# Step 4: Convert to numpy for further analysis\n",
    "#embeddings_np = embeddings.numpy()\n",
    "#print(embeddings_np.shape)  # Shape: [num_sentences, embedding_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6689b46a-5025-469a-ba3a-62c990931ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Reduce embeddings to 3D using PCA for visualization\n",
    "def plot_embedding():\n",
    "    pca = PCA(n_components=3)\n",
    "    embeddings_3d = pca.fit_transform(embeddings_np)  # Shape: [n_docs, 3]\n",
    "    \n",
    "    # Step 3: Define a color palette for unique course departments\n",
    "    unique_departments = alldf['course department2'].unique()\n",
    "    num_departments = len(unique_departments)\n",
    "    #colors = [f\"hsl({i * 360 / num_departments}, 80%, 40%)\" for i in range(num_departments)]\n",
    "    colors = [\n",
    "        \"#1f77b4\",  # Blue\n",
    "        \"#ff7f0e\",  # Orange\n",
    "        \"#2ca02c\",  # Green\n",
    "        \"#d62728\",  # Red\n",
    "        \"#bcbd22\",  # Gold\n",
    "        \"#8c564b\"   # Brown\n",
    "    ]\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add scatter plot, grouping by course department\n",
    "    for i, department in enumerate(unique_departments):\n",
    "        indices = alldf['course department2'] == department\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=embeddings_3d[indices, 0],\n",
    "            y=embeddings_3d[indices, 1],\n",
    "            z=embeddings_3d[indices, 2],\n",
    "            mode='markers',\n",
    "            name=department,  # Use department name as legend\n",
    "            text=alldf.loc[indices, 'Name'],  # Use document names for hover text\n",
    "            hovertemplate=(\n",
    "                \"<b>Document Name:</b> %{text}<br>\" +\n",
    "                \"<b>Department:</b> %{meta}\"  # Include department name in hover\n",
    "            ),\n",
    "            meta=[department for _ in range(sum(indices))],\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                color=colors[i],\n",
    "                line=dict(width=1, color='black')\n",
    "            )\n",
    "        ))\n",
    "    \n",
    "    # Step 5: Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Document Embeddings by Department (PCA-reduced to 3D)\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Principal Component 1\",\n",
    "            yaxis_title=\"Principal Component 2\",\n",
    "            zaxis_title=\"Principal Component 3\"\n",
    "        ),\n",
    "        template=\"plotly\",\n",
    "        showlegend=True,\n",
    "        width=800,  # Width of the plot (in pixels)\n",
    "        height=600,  # Height of the plot (in pixels)\n",
    "        font=dict(\n",
    "            size=9  # Corrected key for setting font size\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Step 6: Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8982578-72ad-4f3b-9ebd-6aeecde802f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "umap_model = UMAP(random_state=42)  # Set random seed for UMAP\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "topic_model1 = BERTopic(umap_model=umap_model,)\n",
    "topic_model2 = BERTopic(vectorizer_model=vectorizer_model)\n",
    "topic_model3 = BERTopic(ctfidf_model=ctfidf_model, top_n_words = 10, calculate_probabilities = True)\n",
    "topic_model4 = BERTopic(vectorizer_model=vectorizer_model,nr_topics = 5, top_n_words = 10)\n",
    "\n",
    "topic_model1.fit(documents)\n",
    "topic_model2.fit(documents)\n",
    "topic_model3.fit(documents)\n",
    "topic_model4.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91ce7a-efb1-49b2-a3c1-24ad5de532e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "topic_model = topic_model4\n",
    "# Assuming 'topic_model' and 'documents' are already defined and transformed\n",
    "topics, probs = topic_model.transform(documents)\n",
    "df = topic_model.get_topic_info()\n",
    "\n",
    "# Define a new CountVectorizer for coherence calculation\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", token_pattern=r\"(?u)\\b[A-Za-z]+\\b\")\n",
    "doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Define coherence score function\n",
    "def coherence_score(topic_words, doc_term_matrix):\n",
    "    scores = []\n",
    "    for words in topic_words:\n",
    "        word_indices = [vectorizer.vocabulary_.get(w) for w in words if w in vectorizer.vocabulary_]\n",
    "        if len(word_indices) > 1:\n",
    "            sub_matrix = doc_term_matrix[:, word_indices].toarray()\n",
    "            similarities = cosine_similarity(sub_matrix.T)\n",
    "            upper_tri_indices = np.triu_indices_from(similarities, k=1)\n",
    "            scores.append(similarities[upper_tri_indices].mean())\n",
    "    return np.mean(scores) if scores else 0\n",
    "\n",
    "# Extract topics and compute coherence scores\n",
    "topics = topic_model.get_topics()  # Get topics as a dictionary\n",
    "topic_coherence_scores = {}\n",
    "\n",
    "for topic_id, topic_words in topics.items():\n",
    "    words = [word for word, _ in topic_words]\n",
    "    topic_coherence_scores[topic_id] = coherence_score([words], doc_term_matrix)\n",
    "\n",
    "# Add coherence scores to the DataFrame\n",
    "df['Coherence Score'] = df['Topic'].map(topic_coherence_scores).round(2)\n",
    "\n",
    "print(df['Coherence Score'].mean().round(2))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45149c8-5003-49a3-a772-42a1c4c5fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Reduce embeddings to 3D using PCA for visualization\n",
    "topic_distribution, _ = topic_model4.approximate_distribution(documents)\n",
    "pca = PCA(n_components=3)\n",
    "embeddings_3d = pca.fit_transform(topic_distribution)  # Shape: [n_docs, 3]\n",
    "\n",
    "# Step 3: Define a color palette for unique course departments\n",
    "unique_departments = alldf['course department2'].unique()\n",
    "num_departments = len(unique_departments)\n",
    "#colors = [f\"hsl({i * 360 / num_departments}, 80%, 40%)\" for i in range(num_departments)]\n",
    "colors = [\n",
    "    \"#1f77b4\",  # Blue\n",
    "    \"#ff7f0e\",  # Orange\n",
    "    \"#2ca02c\",  # Green\n",
    "    \"#d62728\",  # Red\n",
    "    \"#bcbd22\",  # Gold\n",
    "    \"#8c564b\"   # Brown\n",
    "]\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add scatter plot, grouping by course department\n",
    "for i, department in enumerate(unique_departments):\n",
    "    indices = alldf['course department2'] == department\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=embeddings_3d[indices, 0],\n",
    "        y=embeddings_3d[indices, 1],\n",
    "        z=embeddings_3d[indices, 2],\n",
    "        mode='markers',\n",
    "        name=department,  # Use department name as legend\n",
    "        text=alldf.loc[indices, 'Name'],  # Use document names for hover text\n",
    "        hovertemplate=(\n",
    "            \"<b>Document Name:</b> %{text}<br>\" +\n",
    "            \"<b>Department:</b> %{meta}\"  # Include department name in hover\n",
    "        ),\n",
    "        meta=[department for _ in range(sum(indices))],\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=colors[i],\n",
    "            line=dict(width=1, color='black')\n",
    "        )\n",
    "    ))\n",
    "\n",
    "# Step 5: Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Document Embeddings by Department (PCA-reduced to 3D)\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Principal Component 1\",\n",
    "        yaxis_title=\"Principal Component 2\",\n",
    "        zaxis_title=\"Principal Component 3\"\n",
    "    ),\n",
    "    template=\"plotly\",\n",
    "    showlegend=True,\n",
    "    width=800,  # Width of the plot (in pixels)\n",
    "    height=600,  # Height of the plot (in pixels)\n",
    "    font=dict(\n",
    "        size=9  # Corrected key for setting font size\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 6: Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7ad6a-dbab-4595-84ed-ba9d5d326f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_anchor_words_bertopic(model, top_n=100):\n",
    "    \"\"\"\n",
    "    Extract unique anchor words for each topic in a BERTopic model.\n",
    "\n",
    "    Parameters:\n",
    "        model: BERTopic object\n",
    "            A trained BERTopic model.\n",
    "        top_n: int\n",
    "            Number of top words to consider per topic.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are topic IDs and values are lists of unique anchor words.\n",
    "    \"\"\"\n",
    "    topics = model.get_topics()\n",
    "    word_to_topic = {}\n",
    "\n",
    "    # Map each word to the list of topics it appears in\n",
    "    for topic_id, terms in topics.items():\n",
    "        for term, _ in terms[:top_n]:\n",
    "            if term not in word_to_topic:\n",
    "                word_to_topic[term] = set()\n",
    "            word_to_topic[term].add(topic_id)\n",
    "\n",
    "    # Filter words that belong to only one topic\n",
    "    anchor_words = {topic_id: [] for topic_id in topics.keys()}\n",
    "    for word, topic_ids in word_to_topic.items():\n",
    "        if len(topic_ids) == 1:  # The word is unique to one topic\n",
    "            unique_topic = list(topic_ids)[0]\n",
    "            anchor_words[unique_topic].append(word)\n",
    "\n",
    "    return anchor_words\n",
    "find_unique_anchor_words_bertopic(topic_model4 , top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde115a5-4f6b-4c5b-8687-c1061d53b4e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topics, probs = topic_model4.fit_transform(documents)\n",
    "\n",
    "# Step 2: Add topics to DataFrame\n",
    "alldf['Topic'] = topics  # Assign the generated topics to the DataFrame\n",
    "\n",
    "# Step 3: Create a breakdown of topics by department\n",
    "# Group by 'course department' and 'Topic', and count occurrences\n",
    "topic_by_department = alldf.groupby(['course department2', 'Topic']).size().reset_index(name='Count')\n",
    "\n",
    "# Step 4: Pivot to make the data more readable\n",
    "topic_by_department_pivot = topic_by_department.pivot_table(index='course department2', \n",
    "                                                           columns='Topic', \n",
    "                                                           values='Count', \n",
    "                                                           aggfunc='sum', \n",
    "                                                           fill_value=0)\n",
    "\n",
    "# Step 5: Print or visualize the breakdown\n",
    "print(topic_by_department_pivot)\n",
    "\n",
    "# Optionally: If you'd like to visualize the breakdown with a heatmap or bar plot:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(topic_by_department_pivot, annot=True, cmap='viridis', fmt='d')\n",
    "plt.title('Topic Breakdown by Department')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Course Department')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81137fe-659e-4c86-942c-86c4e21e25b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded6e31-04c1-4e88-a863-8bd3dd28bdfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
