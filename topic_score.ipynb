{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a96743-9eaf-4245-97da-77db40adcd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linprog\n",
    "from itertools import combinations\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6fe999-0d84-4b84-b051-6484db48b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(K, K0, m, D, scatterplot=False):\n",
    "    \"\"\"\n",
    "    Translates the given R function to Python.\n",
    "\n",
    "    Parameters:\n",
    "        K: Number of singular values/vectors to compute.\n",
    "        K0: Number of vertices to estimate.\n",
    "        m: Parameter for vertices_est function.\n",
    "        D: Input matrix.\n",
    "        scatterplot: Whether to generate scatterplot.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing A_hat, R, V, Pi, and theta.\n",
    "    \"\"\"\n",
    "    # Dimensions\n",
    "    p = D.shape[0]\n",
    "    \n",
    "    # Step 1: Compute SVD\n",
    "    u, _, _ = svds(D, k=K)\n",
    "    Xi = u\n",
    "    print('Got here 0')\n",
    "\n",
    "    # Make the first column of Xi non-negative\n",
    "    Xi[:, 0] = np.abs(Xi[:, 0])\n",
    "    R = np.apply_along_axis(lambda x: x / Xi[:, 0], axis=0, arr=Xi[:, 1:K])\n",
    "    print('Got here 1')\n",
    "    # Step 2: Estimate vertices\n",
    "    vertices_est_obj = vertices_est(R, K0, m)\n",
    "    V = vertices_est_obj[\"V\"]\n",
    "    theta = vertices_est_obj[\"theta\"]\n",
    "    print('Got here 2')\n",
    "\n",
    "    if scatterplot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.scatter(R[:, 0], R[:, 1])\n",
    "        plt.scatter(V[:, 0], V[:, 1], color='red', s=50)\n",
    "        plt.show()\n",
    "\n",
    "    # Step 3: Compute Pi\n",
    "    Pi = np.hstack([R, np.ones((p, 1))]) @ np.linalg.pinv(np.hstack([V, np.ones((K, 1))]))\n",
    "    Pi = np.maximum(Pi, 0)  # Element-wise max\n",
    "    temp = np.sum(Pi, axis=1, keepdims=True)\n",
    "    Pi = Pi / temp\n",
    "\n",
    "    # Step 4: Compute A_hat\n",
    "    A_hat = Xi[:, [0]] * Pi\n",
    "\n",
    "    # Step 5: Normalize rows of A_hat\n",
    "    temp = np.sum(A_hat, axis=0, keepdims=True)\n",
    "    A_hat = A_hat / temp\n",
    "\n",
    "    return {\"A_hat\": A_hat, \"R\": R, \"V\": V, \"Pi\": Pi, \"theta\": theta}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91162be4-f744-4080-b3b2-015bab569d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vertices_est(R, K0, m):\n",
    "    \"\"\"\n",
    "    Estimate vertices based on the R matrix.\n",
    "\n",
    "    Parameters:\n",
    "        R: Input matrix (n x (K-1)).\n",
    "        K0: Number of vertices to estimate.\n",
    "        m: Number of clusters for k-means.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with `V` (estimated vertices) and `theta` (cluster centers).\n",
    "    \"\"\"\n",
    "    print('Got here')\n",
    "    K = R.shape[1] + 1\n",
    "\n",
    "    # Step 2a: k-means clustering\n",
    "    kmeans = KMeans(n_clusters=m, max_iter=K * 100, n_init=K * 10)\n",
    "    kmeans.fit(R)\n",
    "    theta = kmeans.cluster_centers_\n",
    "    theta_original = np.copy(theta)\n",
    "    \n",
    "    # Optional: Visualization (if needed)\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.scatter(R[:, 0], R[:, 1], alpha=0.5)\n",
    "    # plt.scatter(theta[:, 0], theta[:, 1], color='red', s=50)\n",
    "    # plt.show()\n",
    "\n",
    "    # Step 2b': Find the two furthest apart centers\n",
    "    distance_matrix = cdist(theta, theta, 'sqeuclidean')\n",
    "    top2_indices = np.unravel_index(np.argmax(distance_matrix, axis=None), distance_matrix.shape)\n",
    "    theta0 = theta[list(top2_indices), :]\n",
    "    theta = np.delete(theta, list(top2_indices), axis=0)\n",
    "    \n",
    "    # Step 2b: Select K0 vertices\n",
    "    if K0 > 2:\n",
    "        for _ in range(3, K0 + 1):\n",
    "            distance_to_theta0 = cdist(theta, theta0).mean(axis=1)\n",
    "            max_index = np.argmax(distance_to_theta0)\n",
    "            theta0 = np.vstack([theta0, theta[max_index]])\n",
    "            theta = np.delete(theta, max_index, axis=0)\n",
    "        theta = theta0\n",
    "\n",
    "    # Step 2c: Find optimal combination of K vertices\n",
    "    combs = np.array(list(combinations(range(K0), K)))\n",
    "    max_values = np.zeros(len(combs))\n",
    "\n",
    "    for i, comb in enumerate(combs):\n",
    "        for j in range(K0):\n",
    "            max_values[i] = max(simplex_dist(theta[j], theta[comb]), max_values[i])\n",
    "    \n",
    "    min_index = np.argmin(max_values)\n",
    "\n",
    "    return {\"V\": theta[combs[min_index[0]]], \"theta\": theta_original}\n",
    "\n",
    "\n",
    "def simplex_dist(theta, V):\n",
    "    \"\"\"\n",
    "    Calculate simplex distance between theta and vertices V.\n",
    "\n",
    "    Parameters:\n",
    "        theta: A single point.\n",
    "        V: Vertices of the simplex.\n",
    "\n",
    "    Returns:\n",
    "        Distance value.\n",
    "    \"\"\"\n",
    "    K_minus_1 = V.shape[0] - 1\n",
    "\n",
    "    # Construct VV and D matrices\n",
    "    VV = np.vstack([np.eye(K_minus_1), -np.ones((1, K_minus_1))]) @ V\n",
    "    D = VV @ VV.T\n",
    "    d = VV @ (theta - V[-1])\n",
    "\n",
    "    # Construct inequality constraints\n",
    "    A = np.vstack([np.eye(K_minus_1), -np.ones((1, K_minus_1))])\n",
    "    b = np.zeros(K_minus_1 + 1)\n",
    "    b[-1] = -1\n",
    "\n",
    "    # Solve quadratic programming problem using linear approximation\n",
    "    result = linprog(c=d, A_ub=A, b_ub=b, bounds=(None, None), method='highs')\n",
    "\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization failed!\")\n",
    "\n",
    "    return np.sum((theta - V[-1]) ** 2) + 2 * result.fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d0baeb-4602-4a7e-a03e-f4f0502a1fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mafuangimemkamon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mafuangimemkamon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mafuangimemkamon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2bfcd3-cbc4-4d9b-a9cd-40a2261bcc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IGA 451M - 2020 Spring 1 (170371).pdf</td>\n",
       "      <td>IGA-451M: CONTROVERSIES IN CLIMATE, ENERGY \\n&amp;...</td>\n",
       "      <td>IGA-451M: CONTROVERSIES IN CLIMATE, ENERGY &amp; T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STAT 123 - 2022 Spring (126048).pdf</td>\n",
       "      <td>Thank\\tyou\\tfor\\ttaking\\tStat\\t123\\t2022,\\tQua...</td>\n",
       "      <td>Thank you for taking Stat 123 2022, Quantitati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STAT 364 - 2022 Spring (214539).pdf</td>\n",
       "      <td>Stat364 : Scalable Statistical Inference for B...</td>\n",
       "      <td>Stat364 : Scalable Statistical Inference for B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PSY 2030 - 2021 Fall (160667).pdf</td>\n",
       "      <td>Bayesian Data Analysis\\nPsych 2030 { Fall 2021...</td>\n",
       "      <td>Bayesian Data Analysis Psych 2030 { Fall 2021 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPI 224 - 2024 Fall 2 (190291).pdf</td>\n",
       "      <td>\\nEPI224 Cancer Prevention (2023 Fall 2)  Syl...</td>\n",
       "      <td>EPI224 Cancer Prevention (2023 Fall 2) Syllab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>STAT 171 - 2024 Spring (113721).pdf</td>\n",
       "      <td>Statistics 171: Stochastic Processes, Spring 2...</td>\n",
       "      <td>Statistics 171: Stochastic Processes, Spring 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>STAT 139 - 2024 Fall (110751).pdf</td>\n",
       "      <td>STAT 139: Introduction to Linear Models\\nFall ...</td>\n",
       "      <td>STAT 139: Introduction to Linear Models Fall 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>STAT 221 - 2020 Fall (115077).pdf</td>\n",
       "      <td>Fall 2020\\nSTAT 221: Computational Tools for S...</td>\n",
       "      <td>Fall 2020 STAT 221: Computational Tools for St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>BIOSTAT 241_BST 241 - 2024 Spring (190066).pdf</td>\n",
       "      <td>BST 241/ BIOSTAT 241: Inference II\\n Spring 2...</td>\n",
       "      <td>BST 241/ BIOSTAT 241: Inference II Spring 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>SYSBIO 220QC - 2020 Spring 1 (204480).pdf</td>\n",
       "      <td>SB220/SB221 Syllabus  \\n \\nSB220  \\n12 classes...</td>\n",
       "      <td>SB220/SB221 Syllabus SB220 12 classes Week Dat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           filename  \\\n",
       "0             IGA 451M - 2020 Spring 1 (170371).pdf   \n",
       "1               STAT 123 - 2022 Spring (126048).pdf   \n",
       "2               STAT 364 - 2022 Spring (214539).pdf   \n",
       "3                 PSY 2030 - 2021 Fall (160667).pdf   \n",
       "4                EPI 224 - 2024 Fall 2 (190291).pdf   \n",
       "..                                              ...   \n",
       "242             STAT 171 - 2024 Spring (113721).pdf   \n",
       "245               STAT 139 - 2024 Fall (110751).pdf   \n",
       "246               STAT 221 - 2020 Fall (115077).pdf   \n",
       "247  BIOSTAT 241_BST 241 - 2024 Spring (190066).pdf   \n",
       "248       SYSBIO 220QC - 2020 Spring 1 (204480).pdf   \n",
       "\n",
       "                                                  text  \\\n",
       "0    IGA-451M: CONTROVERSIES IN CLIMATE, ENERGY \\n&...   \n",
       "1    Thank\\tyou\\tfor\\ttaking\\tStat\\t123\\t2022,\\tQua...   \n",
       "2    Stat364 : Scalable Statistical Inference for B...   \n",
       "3    Bayesian Data Analysis\\nPsych 2030 { Fall 2021...   \n",
       "4     \\nEPI224 Cancer Prevention (2023 Fall 2)  Syl...   \n",
       "..                                                 ...   \n",
       "242  Statistics 171: Stochastic Processes, Spring 2...   \n",
       "245  STAT 139: Introduction to Linear Models\\nFall ...   \n",
       "246  Fall 2020\\nSTAT 221: Computational Tools for S...   \n",
       "247   BST 241/ BIOSTAT 241: Inference II\\n Spring 2...   \n",
       "248  SB220/SB221 Syllabus  \\n \\nSB220  \\n12 classes...   \n",
       "\n",
       "                                          cleaned_text  \n",
       "0    IGA-451M: CONTROVERSIES IN CLIMATE, ENERGY & T...  \n",
       "1    Thank you for taking Stat 123 2022, Quantitati...  \n",
       "2    Stat364 : Scalable Statistical Inference for B...  \n",
       "3    Bayesian Data Analysis Psych 2030 { Fall 2021 ...  \n",
       "4     EPI224 Cancer Prevention (2023 Fall 2) Syllab...  \n",
       "..                                                 ...  \n",
       "242  Statistics 171: Stochastic Processes, Spring 2...  \n",
       "245  STAT 139: Introduction to Linear Models Fall 2...  \n",
       "246  Fall 2020 STAT 221: Computational Tools for St...  \n",
       "247   BST 241/ BIOSTAT 241: Inference II Spring 202...  \n",
       "248  SB220/SB221 Syllabus SB220 12 classes Week Dat...  \n",
       "\n",
       "[148 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processed_data/stat_courses.csv')\n",
    "df = df[df['filename'].str.contains('STAT')|df['text'].str.lower().str.contains('statistics')]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1d516f-ce36-4c54-984c-0b02e6920ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (148, 100)\n",
      "First 5 feature names: ['academic' 'al' 'also' 'analysis' 'assignment']\n",
      "   academic        al      also  analysis  assignment  available    canvas  \\\n",
      "0  0.004526  0.023590  0.011909  0.008162    0.026299   0.001877  0.004128   \n",
      "1  0.000000  0.000000  0.000000  0.000000    0.000000   0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  0.077060    0.000000   0.000000  0.000000   \n",
      "3  0.052626  0.000000  0.039560  0.037959    0.000000   0.000000  0.047994   \n",
      "4  0.071688  0.105077  0.033681  0.025854    0.110638   0.029730  0.089896   \n",
      "\n",
      "       care      case  causal  ...      time      tool     topic       two  \\\n",
      "0  0.000000  0.002228     0.0  ...  0.083735  0.000000  0.009213  0.007298   \n",
      "1  0.000000  0.000000     0.0  ...  0.086658  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000     0.0  ...  0.076013  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.000000     0.0  ...  0.037444  0.000000  0.107108  0.000000   \n",
      "4  0.026974  0.017642     0.0  ...  0.057382  0.033947  0.079032  0.050568   \n",
      "\n",
      "   university       use     using       via      week      work  \n",
      "0    0.020695  0.013705  0.001774  0.004599  0.005069  0.010068  \n",
      "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2    0.000000  0.000000  0.083735  0.000000  0.000000  0.000000  \n",
      "3    0.000000  0.079670  0.000000  0.053469  0.039287  0.078035  \n",
      "4    0.036418  0.061047  0.049164  0.018209  0.080277  0.086369  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming `df['cleaned_text']` contains the documents\n",
    "documents = df['cleaned_text'].values\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]  # Lemmatize and remove non-alphabetic tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stop words\n",
    "    return ' '.join(tokens)  # Join tokens back into a single string\n",
    "\n",
    "# Preprocess each document\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Convert documents to a TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_features=100)  # Keep only top 1000 features\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "# Feature names and matrix shape\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "matrix_shape = tfidf_matrix.shape\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {matrix_shape}\")\n",
    "print(f\"First 5 feature names: {feature_names[:5]}\")\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc235f0-dc7c-4857-a9db-3769769fe250",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(6, 3, 3, tfidf_df.values, scatterplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c97467-60d8-4f31-ace6-59fe9c71847d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bef051-9f6a-437a-90eb-087621aebccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa6aca-96fe-4f52-bee4-3791600d7c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9faeeac-2303-4861-ae41-247b63d31ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
